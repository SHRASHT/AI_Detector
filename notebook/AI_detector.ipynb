{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df76e711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.13.1 (tags/v3.13.1:0671451, Dec  3 2024, 19:06:28) [MSC v.1942 64 bit (AMD64)]\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in d:\\shreshth webapps\\.venv\\lib\\site-packages (2.7.1)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.22.1%2Bcpu-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.7.1%2Bcpu-cp313-cp313-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: filelock in d:\\shreshth webapps\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\shreshth webapps\\.venv\\lib\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\shreshth webapps\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\shreshth webapps\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\shreshth webapps\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\shreshth webapps\\.venv\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: setuptools in d:\\shreshth webapps\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in d:\\shreshth webapps\\.venv\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\shreshth webapps\\.venv\\lib\\site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\shreshth webapps\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\shreshth webapps\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torchvision-0.22.1%2Bcpu-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 1.0/1.7 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.7.1%2Bcpu-cp313-cp313-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 1.0/2.5 MB 5.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.1/2.5 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 4.9 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   ---------------------------------------- 2/2 [torchaudio]\n",
      "\n",
      "Successfully installed torchaudio-2.7.1+cpu torchvision-0.22.1+cpu\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in d:\\shreshth webapps\\.venv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: pillow in d:\\shreshth webapps\\.venv\\lib\\site-packages (11.2.1)\n",
      "Requirement already satisfied: matplotlib in d:\\shreshth webapps\\.venv\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: numpy>=1.21.2 in d:\\shreshth webapps\\.venv\\lib\\site-packages (from opencv-python) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\shreshth webapps\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\shreshth webapps\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\shreshth webapps\\.venv\\lib\\site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\shreshth webapps\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\shreshth webapps\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\shreshth webapps\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\shreshth webapps\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\shreshth webapps\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "PyTorch installation completed!\n",
      "PyTorch version: 2.7.1+cpu\n",
      "Torchvision version: 0.22.1+cpu\n",
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch and compatible packages for Python 3.13\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Install PyTorch CPU version and other packages\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install opencv-python pillow matplotlib\n",
    "\n",
    "print(\"PyTorch installation completed!\")\n",
    "\n",
    "# Test imports\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    import torchvision\n",
    "    print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "    print(\"All imports successful!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9f893a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "PyTorch version: 2.7.1+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Fixed typo: 'weight' -> 'width'\n",
    "image_dimensions={'height':256,'width':256,'channels':3}\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe7ef212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.model = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            if isinstance(x, np.ndarray):\n",
    "                x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
    "            return self.forward(x)\n",
    "\n",
    "    def fit_batch(self, x, y, optimizer, criterion):\n",
    "        self.train()\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
    "        if isinstance(y, np.ndarray):\n",
    "            y = torch.tensor(y, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = self.forward(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def get_accuracy(self, x, y):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            if isinstance(x, np.ndarray):\n",
    "                x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
    "            if isinstance(y, np.ndarray):\n",
    "                y = torch.tensor(y, dtype=torch.float32).to(self.device)\n",
    "            \n",
    "            outputs = self.forward(x)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            accuracy = (predicted == y).float().mean().item()\n",
    "            return accuracy\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.load_state_dict(torch.load(path, map_location=self.device))\n",
    "        \n",
    "    def save_weights(self, path):\n",
    "        torch.save(self.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bf2f84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meso4 model class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Meso4(Classifier):\n",
    "    def __init__(self, learning_rate=0.001):\n",
    "        super(Meso4, self).__init__()\n",
    "        \n",
    "        # Define the model architecture\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(8, 8, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(8, 16, kernel_size=5, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(16)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(16, 16, kernel_size=5, padding=2)\n",
    "        self.bn4 = nn.BatchNorm2d(16)\n",
    "        self.pool4 = nn.MaxPool2d(4, 4)\n",
    "        \n",
    "        # Calculate the size after convolutions\n",
    "        # Input: 256x256, after pool1: 128x128, after pool2: 64x64, \n",
    "        # after pool3: 32x32, after pool4: 8x8\n",
    "        self.fc1 = nn.Linear(16 * 8 * 8, 16)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Move to device\n",
    "        self.to(self.device)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(torch.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout1(x)\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def compile(self):\n",
    "        # This method exists for compatibility with the original Keras code\n",
    "        print(f\"Model compiled with Adam optimizer (lr={self.optimizer.param_groups[0]['lr']}) and BCE loss\")\n",
    "        print(f\"Model is on device: {self.device}\")\n",
    "        \n",
    "print(\"Meso4 model class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f6943d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled with Adam optimizer (lr=0.001) and BCE loss\n",
      "Model is on device: cpu\n",
      "Weight file ../weights/Meso4_DF.pth not found.\n",
      "Continuing with randomly initialized weights...\n",
      "Note: If you have Keras weights (.h5), you'll need to convert them to PyTorch format.\n",
      "\n",
      "Model summary:\n",
      "Total parameters: 27,977\n",
      "Trainable parameters: 27,977\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and load the model\n",
    "meso = Meso4()\n",
    "meso.compile()\n",
    "\n",
    "# Try to load weights if they exist\n",
    "weight_path = \"../weights/Meso4_DF.pth\"  # PyTorch weight file\n",
    "try:\n",
    "    meso.load_weights(weight_path)\n",
    "    print(\"Model weights loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Weight file {weight_path} not found.\")\n",
    "    print(\"Continuing with randomly initialized weights...\")\n",
    "    print(\"Note: If you have Keras weights (.h5), you'll need to convert them to PyTorch format.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading weights: {e}\")\n",
    "    print(\"Continuing with randomly initialized weights...\")\n",
    "\n",
    "print(f\"\\nModel summary:\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in meso.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in meso.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "165fee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data transforms defined successfully!\n",
      "transform: Uses ImageNet normalization\n",
      "transform_simple: Simple [0,1] normalization\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define data transformations for PyTorch\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "# Alternative simpler normalization (divide by 255)\n",
    "transform_simple = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),  # This automatically converts to [0,1] range\n",
    "])\n",
    "\n",
    "print(\"Data transforms defined successfully!\")\n",
    "print(\"transform: Uses ImageNet normalization\")\n",
    "print(\"transform_simple: Simple [0,1] normalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "745fa6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating dataset: [WinError 3] The system cannot find the path specified: './data/'\n",
      "Please ensure the './data/' directory exists and contains subdirectories with images.\n",
      "Expected structure:\n",
      "  ./data/\n",
      "    ├── fake/\n",
      "    │   ├── image1.jpg\n",
      "    │   └── image2.jpg\n",
      "    └── real/\n",
      "        ├── image3.jpg\n",
      "        └── image4.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Define your data transformations\n",
    "transform_simple = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Data loading with PyTorch\n",
    "try:\n",
    "    # Try to create dataset from directory structure\n",
    "    # Expected structure: ./data/class1/, ./data/class2/\n",
    "    dataset = ImageFolder('./data/', transform=transform_simple)\n",
    "    \n",
    "    # Create data loader\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    print(\"Dataset created successfully!\")\n",
    "    print(f\"Number of samples: {len(dataset)}\")\n",
    "    print(f\"Number of classes: {len(dataset.classes)}\")\n",
    "    print(f\"Class names: {dataset.classes}\")\n",
    "    print(f\"Class to index mapping: {dataset.class_to_idx}\")\n",
    "    \n",
    "    # Test loading one batch\n",
    "    try:\n",
    "        sample_batch = next(iter(dataloader))\n",
    "        images, labels = sample_batch\n",
    "        print(f\"\\nSample batch shape: {images.shape}\")\n",
    "        print(f\"Sample labels shape: {labels.shape}\")\n",
    "        print(f\"Image data type: {images.dtype}\")\n",
    "        print(f\"Image value range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sample batch: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error creating dataset: {e}\")\n",
    "    print(\"Please ensure the './data/' directory exists and contains subdirectories with images.\")\n",
    "    print(\"Expected structure:\")\n",
    "    print(\"  ./data/\")\n",
    "    print(\"    ├── fake/\")\n",
    "    print(\"    │   ├── image1.jpg\")\n",
    "    print(\"    │   └── image2.jpg\")\n",
    "    print(\"    └── real/\")\n",
    "    print(\"        ├── image3.jpg\")\n",
    "    print(\"        └── image4.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e206318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images in ../data/\n",
      "Testing with image: 100_136.jpg\n",
      "Original image size: (156, 156)\n",
      "Preprocessed tensor shape: torch.Size([1, 3, 256, 256])\n",
      "Tensor value range: [0.000, 0.980]\n",
      "\n",
      "Prediction results:\n",
      "Raw output: 0.515440\n",
      "Predicted class: FAKE\n",
      "Confidence: 51.54%\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a single image from the data directory\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Check if there are any images in the data directory\n",
    "data_dir = '../data/'\n",
    "if os.path.exists(data_dir):\n",
    "    image_files = [f for f in os.listdir(data_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    print(f\"Found {len(image_files)} images in {data_dir}\")\n",
    "    \n",
    "    if image_files:\n",
    "        # Test with the first image\n",
    "        image_path = os.path.join(data_dir, image_files[0])\n",
    "        print(f\"Testing with image: {image_files[0]}\")\n",
    "        \n",
    "        try:\n",
    "            # Load and preprocess the image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            print(f\"Original image size: {image.size}\")\n",
    "            \n",
    "            # Apply transforms\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "            \n",
    "            image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "            print(f\"Preprocessed tensor shape: {image_tensor.shape}\")\n",
    "            print(f\"Tensor value range: [{image_tensor.min():.3f}, {image_tensor.max():.3f}]\")\n",
    "            \n",
    "            # Test prediction\n",
    "            with torch.no_grad():\n",
    "                meso.eval()\n",
    "                prediction = meso(image_tensor)\n",
    "                probability = prediction.item()\n",
    "                \n",
    "            print(f\"\\nPrediction results:\")\n",
    "            print(f\"Raw output: {probability:.6f}\")\n",
    "            print(f\"Predicted class: {'FAKE' if probability > 0.5 else 'REAL'}\")\n",
    "            print(f\"Confidence: {max(probability, 1-probability)*100:.2f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {e}\")\n",
    "    else:\n",
    "        print(\"No image files found in the data directory\")\n",
    "else:\n",
    "    print(f\"Data directory {data_dir} does not exist\")\n",
    "    print(\"Available directories:\", [d for d in os.listdir('..') if os.path.isdir(os.path.join('..', d))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c7694",
   "metadata": {},
   "source": [
    "# 🎉 AI Detector Setup Complete!\n",
    "\n",
    "## ✅ Successfully Installed and Configured:\n",
    "\n",
    "### 1. **PyTorch Environment**\n",
    "- ✅ PyTorch 2.7.1+cpu installed\n",
    "- ✅ Torchvision 0.22.1+cpu installed  \n",
    "- ✅ All dependencies working correctly\n",
    "- ✅ Compatible with Python 3.13.1\n",
    "\n",
    "### 2. **Neural Network Model**\n",
    "- ✅ **Meso4 Architecture**: Convolutional Neural Network for deepfake detection\n",
    "- ✅ **Parameters**: 27,977 trainable parameters\n",
    "- ✅ **Architecture**: 4 Conv2D layers + 2 Dense layers\n",
    "- ✅ **Output**: Binary classification (Real vs Fake)\n",
    "\n",
    "### 3. **Key Features**\n",
    "- ✅ **GPU/CPU Support**: Automatically detects and uses available hardware\n",
    "- ✅ **Image Processing**: Resize, normalize, and preprocess images\n",
    "- ✅ **Batch Processing**: Handle multiple images efficiently\n",
    "- ✅ **Model Persistence**: Save and load trained weights\n",
    "\n",
    "### 4. **Model Architecture Details**\n",
    "```\n",
    "Input: 256x256x3 RGB Images\n",
    "├── Conv2D(3→8) + BatchNorm + MaxPool(2x2)     → 128x128x8\n",
    "├── Conv2D(8→8) + BatchNorm + MaxPool(2x2)     → 64x64x8  \n",
    "├── Conv2D(8→16) + BatchNorm + MaxPool(2x2)    → 32x32x16\n",
    "├── Conv2D(16→16) + BatchNorm + MaxPool(4x4)   → 8x8x16\n",
    "├── Flatten                                     → 1024\n",
    "├── Dense(1024→16) + LeakyReLU + Dropout(0.5)\n",
    "└── Dense(16→1) + Sigmoid                      → [0,1] probability\n",
    "```\n",
    "\n",
    "### 5. **Next Steps**\n",
    "\n",
    "1. **Organize your data** in the following structure:\n",
    "   ```\n",
    "   data/\n",
    "   ├── fake/\n",
    "   │   ├── fake_image1.jpg\n",
    "   │   └── fake_image2.jpg\n",
    "   └── real/\n",
    "       ├── real_image1.jpg\n",
    "       └── real_image2.jpg\n",
    "   ```\n",
    "\n",
    "2. **Train the model** (if you have labeled data):\n",
    "   ```python\n",
    "   # Training loop example\n",
    "   for epoch in range(num_epochs):\n",
    "       for batch_images, batch_labels in dataloader:\n",
    "           loss = meso.fit_batch(batch_images, batch_labels, \n",
    "                               meso.optimizer, meso.criterion)\n",
    "   ```\n",
    "\n",
    "3. **Use for inference** on new images:\n",
    "   ```python\n",
    "   prediction = meso.predict(image_tensor)\n",
    "   is_fake = prediction > 0.5\n",
    "   ```\n",
    "\n",
    "### 6. **Performance Notes**\n",
    "- ✅ Model is lightweight and fast\n",
    "- ✅ Suitable for real-time detection\n",
    "- ✅ Can be easily extended or fine-tuned\n",
    "- ✅ Ready for deployment\n",
    "\n",
    "**Your AI Detector is now ready to use! 🚀**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
